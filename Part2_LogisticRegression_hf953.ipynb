{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "In the assignment, you will use gradient ascent to find the weights for the logistic regression.   \n",
    "\n",
    "As an example, we will use the widely-used breast cancer data set.  This data set is described here:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, preprocessing, and understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "# Import breastcancer dataset\n",
    "# Import preprocessing from sklearn\n",
    "# Import train_test_split from sklearn\n",
    "# Import numpy,math\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to a python variable cancer\n",
    "# Store target to a variable called y\n",
    "# Store feature to a variable called X\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "cancer=load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of data (X) and target (Y) values \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "#### Splitting the data into train and test before scaling the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split() function to split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# Store the return value of pervious step to X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data since we will be using gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scaler of the dataset by using preprocessing.StandardScaler().fit()\n",
    "# Using this scale to scale the X_train and X_test using .transform()\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426,)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Print the shape of x_train and y_train \n",
    "# print(X_train.shape) # It should print (426, 30)\n",
    "# print(y_train.shape) # It should print (426,)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a column of ones to the  matrices $X_{train}$ and  $X_{test}$\n",
    "After adding a column of ones $X_{train}=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$\n",
    "\n",
    "Similarly for $X_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trainng data has dimensions:  (426, 31) . The testing data has dimensions:  (143, 31)\n",
      "[[ 1.         -0.34913849 -1.43851335 ... -0.91671059 -0.92508585\n",
      "  -0.80841115]\n",
      " [ 1.         -0.20468665  0.31264011 ...  1.43655962  1.14955889\n",
      "   1.56911143]\n",
      " [ 1.         -0.32931176 -0.21507235 ... -0.7237126   0.53496977\n",
      "  -0.61934827]\n",
      " ...\n",
      " [ 1.          0.04739597 -0.56293662 ... -1.23262438 -0.68282718\n",
      "  -1.261137  ]\n",
      " [ 1.         -0.04040808  0.09966199 ...  1.08847951  0.48944465\n",
      "   1.26159953]\n",
      " [ 1.         -0.5502381   0.31264011 ... -0.59582424 -0.29911546\n",
      "  -0.82948141]]\n",
      "[[ 1.         -0.34913849 -1.43851335 -0.41172595 -0.39047943 -1.86366229\n",
      "  -1.26860704 -0.82617052 -0.95286585 -1.72936805 -0.9415409  -0.86971355\n",
      "  -1.35865347 -0.83481506 -0.57230673 -0.74586846 -0.65398319 -0.52583524\n",
      "  -0.94677147 -0.53781728 -0.63449458 -0.54268486 -1.65565452 -0.58986401\n",
      "  -0.52555985 -1.51066925 -0.89149994 -0.75021715 -0.91671059 -0.92508585\n",
      "  -0.80841115]\n",
      " [ 1.         -0.20468665  0.31264011 -0.13367256 -0.27587995  1.07807258\n",
      "   0.86354605  0.72631375  0.89844062  1.17876963  1.47437716 -0.04022275\n",
      "  -0.50962253  0.10947722 -0.13472838 -0.52489487 -0.14934475  0.07460028\n",
      "   0.23747244 -0.43028253  0.08289146  0.04148684  0.68989862  0.19412774\n",
      "  -0.05193356  1.12941497  0.92394223  1.22221738  1.43655962  1.14955889\n",
      "   1.56911143]]\n",
      "(426,)\n"
     ]
    }
   ],
   "source": [
    "# Append a column of ones to x_train \n",
    "ones = np.ones((X_train.shape[0],1)) \n",
    "X_train = np.hstack((ones,X_train))\n",
    "# Create a column vector of ones by using np.ones and reshape\n",
    "# Append a column of ones in the beginning of x_train by using np.hstack\n",
    "\n",
    "# Now do the same for the test data\n",
    "ones = np.ones((X_test.shape[0], 1)) \n",
    "X_test = np.hstack((ones,X_test))\n",
    "\n",
    "# We can check that everything worked correctly by:\n",
    "# Printing out the new dimensions\n",
    "print(\"The trainng data has dimensions: \", X_train.shape, \". The testing data has dimensions: \",X_test.shape)\n",
    "# # Looking at the first two rows of X_train to check everything worked as expected\n",
    "print(X_train)\n",
    "print(X_train[0:2])\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the names of all the features\n",
    "# print(cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add your own code here to better understand the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Before writing the gradient ascent code, first write some helpful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "### Sigmoid($z$)\n",
    "The first function you will write is sigmoid($z$)\n",
    "\n",
    "sigmoid($z$) takes as input a column vector of real numbers, $z^T = [z_1, z_2, ..., z_{N'}]$, where $N'$ is the number of  examples\n",
    "\n",
    "It should produce as output a column vector $\\left[\\frac{1}{1+e^{-z_1}},\\frac{1}{1+e^{-z_2}},...,\\frac{1}{1+e^{-z_{N'}}}\\right]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sigmoid function\n",
    "def sigmoid(z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing ${\\bf w}$\n",
    "For testing the next functions, we create a coefficient vector, ${\\bf w}$.\n",
    "We will initialize the coeffients to be $0$, i.e. ${\\bf w}^T = [0,0,\\ldots ,0]$ (We could have initialized ${\\bf w}$ to any values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize w using np.zeros()\n",
    "\n",
    "w=np.zeros((X_train.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our hypothesis, $h({\\bf x})$\n",
    "The next  function to write is our hypothesis function. \n",
    "\n",
    "For example if our design matrix $X$ consists of single example $X=[1,x_1,x_2,\\ldots,x_d]$ and  weights ${\\bf w}^T=[w_0,w_2,\\ldots, w_d]$, it returns $h({\\bf x})=\\frac{1}{1+e^{-\\left({w_{0}\\cdot 1 +w_1\\cdot x_1+\\cdots w_d\\cdot x_d}\\right)}}$\n",
    "\n",
    "If given a  matrix consisting of $N'$ examples \n",
    "$X=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$\n",
    "and  weights ${\\bf w}^T=[w_0,w_2,\\ldots, w_d]$, the function returns a column vector\n",
    "$[h({\\bf x}^{(1)}),h({\\bf x}^{(2)},\\ldots, h({\\bf x}^{(N')}]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probability that a patient has cancer \n",
    "# Write the hypothesis function \n",
    "def hypothesis(X , w):\n",
    "    h=sigmoid(np.dot(X, w))\n",
    "    return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.zeros((X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood Function.\n",
    "Write the code to calculate the log likelihood function $\\ell({\\bf w})=\n",
    "\\sum_{i=1}^{N'}y^{(i)}\\ln(h({\\bf x}^{(i)})) +(1- y^{(i)})\\ln(1-h({\\bf x}^{(i)}))$\n",
    "\n",
    "The input is a matrix consisting of $N'$ examples $X=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$\n",
    "and a column vector ${\\bf y}^T=[y^{(1)},y^{(2)},\\dots,y^{(N')}]$ of labels for $X$.\n",
    "\n",
    "The output is $\\ell({\\bf w})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the log likelihood function \n",
    "def log_likelihood(X , y , w ):\n",
    "    s = np.dot(X, w)\n",
    "    like = np.sum( y*s - np.log(1 + np.exp(s)) )\n",
    "    return like\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent\n",
    "Now write the code to perform gradient ascent.  You will use the update rule from the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Write the gradient ascent function \n",
    "def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters):\n",
    "    # For every 100 iterations, store the log_likelihood for the current w\n",
    "    # Initializing log_likelihood to be an empty list  \n",
    "    # Initialize w to be a zero vector of shape x_train.shape[1],1\n",
    "    # Initialize N to the number of training examples\n",
    "    log_likelihood_values=[]\n",
    "    w=np.zeros((X.shape[1],1))\n",
    "#     print(w)\n",
    "    N=X.shape[0]\n",
    "    y=y.reshape(X.shape[0],1)\n",
    "    for i in range(num_iters):\n",
    "        h=hypothesis(X,w)\n",
    "        gradient = (1 / N) * np.matmul(X.T, (y - h))\n",
    "        w = w+(learning_rate * gradient)\n",
    "        if i % 100 == 0:\n",
    "            val =log_likelihood(X, y, w)\n",
    "            log_likelihood_values.append(val)\n",
    "            print('Iteration Number: ',i,'------','Log Likelihood Value: ',val)\n",
    "        \n",
    "        # update the w using formula \n",
    "        # append the log_likelihodd values to the list for every 100 iterations\n",
    "        \n",
    "    return w ,log_likelihood_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After completing the code above, run the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number:  0 ------ Log Likelihood Value:  -287.13806310797196\n",
      "Iteration Number:  100 ------ Log Likelihood Value:  -109.07046636871948\n",
      "Iteration Number:  200 ------ Log Likelihood Value:  -82.81594468809607\n",
      "Iteration Number:  300 ------ Log Likelihood Value:  -70.81316442317494\n",
      "Iteration Number:  400 ------ Log Likelihood Value:  -63.643249284976605\n",
      "Iteration Number:  500 ------ Log Likelihood Value:  -58.76677404827597\n",
      "Iteration Number:  600 ------ Log Likelihood Value:  -55.18000266634923\n",
      "Iteration Number:  700 ------ Log Likelihood Value:  -52.40034899283923\n",
      "Iteration Number:  800 ------ Log Likelihood Value:  -50.164641435551985\n",
      "Iteration Number:  900 ------ Log Likelihood Value:  -48.31593009823706\n",
      "Iteration Number:  1000 ------ Log Likelihood Value:  -46.75412903198439\n",
      "Iteration Number:  1100 ------ Log Likelihood Value:  -45.412012215271886\n",
      "Iteration Number:  1200 ------ Log Likelihood Value:  -44.24251991505457\n",
      "Iteration Number:  1300 ------ Log Likelihood Value:  -43.21158412479137\n",
      "Iteration Number:  1400 ------ Log Likelihood Value:  -42.29384744745689\n",
      "Iteration Number:  1500 ------ Log Likelihood Value:  -41.46999084329319\n",
      "Iteration Number:  1600 ------ Log Likelihood Value:  -40.725001074164695\n",
      "Iteration Number:  1700 ------ Log Likelihood Value:  -40.04701040089549\n",
      "Iteration Number:  1800 ------ Log Likelihood Value:  -39.426497497389995\n",
      "Iteration Number:  1900 ------ Log Likelihood Value:  -38.855723582416445\n",
      "Iteration Number:  2000 ------ Log Likelihood Value:  -38.32832595290944\n",
      "Iteration Number:  2100 ------ Log Likelihood Value:  -37.83901940848294\n",
      "Iteration Number:  2200 ------ Log Likelihood Value:  -37.383373224731805\n",
      "Iteration Number:  2300 ------ Log Likelihood Value:  -36.9576420452087\n",
      "Iteration Number:  2400 ------ Log Likelihood Value:  -36.55863591810241\n",
      "Iteration Number:  2500 ------ Log Likelihood Value:  -36.18361919317525\n",
      "Iteration Number:  2600 ------ Log Likelihood Value:  -35.830230995717656\n",
      "Iteration Number:  2700 ------ Log Likelihood Value:  -35.49642203853294\n",
      "Iteration Number:  2800 ------ Log Likelihood Value:  -35.180403949407264\n",
      "Iteration Number:  2900 ------ Log Likelihood Value:  -34.880608288423474\n",
      "Iteration Number:  3000 ------ Log Likelihood Value:  -34.595653141242806\n",
      "Iteration Number:  3100 ------ Log Likelihood Value:  -34.32431568941816\n",
      "Iteration Number:  3200 ------ Log Likelihood Value:  -34.06550953591223\n",
      "Iteration Number:  3300 ------ Log Likelihood Value:  -33.818265843300765\n",
      "Iteration Number:  3400 ------ Log Likelihood Value:  -33.58171755118529\n",
      "Iteration Number:  3500 ------ Log Likelihood Value:  -33.35508609733127\n",
      "Iteration Number:  3600 ------ Log Likelihood Value:  -33.137670187541254\n",
      "Iteration Number:  3700 ------ Log Likelihood Value:  -32.92883625195485\n",
      "Iteration Number:  3800 ------ Log Likelihood Value:  -32.72801029732457\n",
      "Iteration Number:  3900 ------ Log Likelihood Value:  -32.53467092094729\n",
      "Iteration Number:  4000 ------ Log Likelihood Value:  -32.34834329608551\n",
      "Iteration Number:  4100 ------ Log Likelihood Value:  -32.16859397367679\n",
      "Iteration Number:  4200 ------ Log Likelihood Value:  -31.995026372990942\n",
      "Iteration Number:  4300 ------ Log Likelihood Value:  -31.827276856225872\n",
      "Iteration Number:  4400 ------ Log Likelihood Value:  -31.665011300034323\n",
      "Iteration Number:  4500 ------ Log Likelihood Value:  -31.507922091560616\n",
      "Iteration Number:  4600 ------ Log Likelihood Value:  -31.35572548844675\n",
      "Iteration Number:  4700 ------ Log Likelihood Value:  -31.208159291990537\n",
      "Iteration Number:  4800 ------ Log Likelihood Value:  -31.06498079063079\n",
      "Iteration Number:  4900 ------ Log Likelihood Value:  -30.92596493753601\n"
     ]
    }
   ],
   "source": [
    "# Set the learning_rate\n",
    "# Set the num_iters\n",
    "# Run the Logistic_Regresion_Gradient_Ascent() and store the returned values\n",
    "w_final,log_likelihood_values= Logistic_Regresion_Gradient_Ascent(X_train, y_train, 0.01, 5000)\n",
    "# print(w_final)\n",
    "\n",
    "# cancer = LogisticRegression(C=1e20)\n",
    "# cancer.fit(X_train, y_train)\n",
    "# cancer.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting Likelihood v/s Number of Iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuc0lEQVR4nO3de3xU1bn/8c+TEEIAFRWMyC0g3rEiUgWPaLzUS7VqWz0F7UFrj9Ye26rteXm0eKy12tb6a09bW4/V1qpVi1aPBa+IaFAxioCIoHKRi4AoCggkQK7P74+9ZhySmUwyZDK5fN+v17wys/btWcOwn1lr7Vnb3B0REZFM5OU6ABER6biUREREJGNKIiIikjElERERyZiSiIiIZExJREREMqYk0gWZ2TgzW5zweqWZnZLBfuLbmdmPzezP4XmJmbmZdWu9qFPGUGZm/57t47SlTP89WunYxWb2kpltNbNf5yKGdMzsGTO7KNdxSERJpBNLdTJy95fd/aDWPJa7/9zdO9XJHHZKiE83KH/AzG7MUVjZdBnwKbC7u/+o4UIzu9fMbg7Ps/5lwcxuNLMHEsvc/Qx3vy9bx5SWURIRaZ5jzOzYXAfREhme3IcA73gb/Aq5LVqqkn1KIl2QmZWa2ZoUyw4xsxVmNiG8PsvM5pvZZ2b2qpl9IcV2jb4xAhea2Qdm9qmZTUpYt9DMfmtmH4bHb82sMGH5pWa2zMw2mtlUM9svYdmXzOw9M9tsZn8ALEU8+5nZdjPbK6HsyBBLgZkNN7OZYT+fmtnDad62XwG3pDjWxWb2SoMyN7Ph4fm9ZnZH6IapMLNZZrZvqPemUJ8jG+z2i2b2Tlj+VzPrkbDvlP8mofX5X2a2AKhMdqI2s2PN7I1Q9zdiydHM7gUuAq4JcabrUnsp/P0srD827OcSM3s3xD7NzIY0eF+uMLOlwNJQ9jszW21mW8xsrpmNC+WnAz8GvhH2/1Yoj3dhmlmemV1vZqvMbL2Z3W9me4RlsZbSRSk+h0eb2Zxw3I/N7Ddp6ivJuLsenfQBrAROSVJeCqxpuB4wCvgAOCuUHwmsB44B8olOMCuBwob7B24EHgjPSwAH7gaKgCOAKuCQsPwm4DVgH6Af8Crws7DsJKLulFFAIXA78FJY1hfYCpwHFABXA7XAv6eo/wvApQmvbwPuDM//Dkwi+iLVAzguxT5iddkNWJtQ3weAG8Pzi4FXGmznwPDw/N5Qp6PCsV4AVgATw/t6M/Big3+PhcAgYC9gFnBzC/5N5odti5LUZy9gE/BvQDdgQni9d0KsNzfxmbo3IZbYe9MtYfk5wDLgkLD/64FXG7wv00McRaHsm8DeYf0fAR8BPRp+rhL2URb7NwcuCccbBvQG/g/4WzM/h+XAv4XnvYExuf4/2xEfaolIzDhgKjDR3Z8MZZcBf3L31929zqN+6CpgTDP3+VN33+7ubwFvEf0nBrgQuMnd17v7J8BPiU5qsWX3uPs8d68CrgPGmlkJ8GVgkbs/6u41wG+JTjipPER0ksTMDBgfygBqiLpu9nP3He7+SvJdxG0naonc3LyqN/K4u8919x3A48AOd7/f3euAh4mSQ6I/uPtqd98YjjshlDfn3+T3YdvtSeI4E1jq7n9z91p3/zvwHvCVDOvV0OXAL9z9XXevBX4OjExsjYTlG2PxufsD7r4hxPNroi8PzR2zuxD4jbsvd/cKos/L+AYtsFSfwxpguJn1dfcKd38t41p3YUoiEnM50TfGsoSyIcCPQrfJZ2b2GdE33P2SbJ9M4gl+G9G3PcL2qxKWrUrY507LwolhAzAgLFudsMwTXyfxGFEC6g8cD9QDL4dl1xB1hc02s0Vmdkkz6vNnoNjMMjnhfpzwfHuS1713Xn2neiW+P835N2nqPWn43sf2P6DJ6JtvCPC7hNg2Er3PifvfKT4z+8/Q/bU5bLMHUauzOZJ9lroBxQllqT6H3wYOBN4L3XpnNfOYkkBJRGIuBwab2f8klK0GbnH3PgmPnuHb6674kOhkEzM4lDVaZma9iLo61gLriE6YsWWW+Lohd98EPAd8A7gAmBwSD+7+kbtf6u77Ad8B7oiNYTSxv2qiVtPP2HksphLomRDXvk3tp5kS65X4/jTn36SpQfGG731s/2sziDHZcVYD32kQX5G7v5psuzD+cQ3wr8Ce7t4H2Mzn72+6Af5kn6Vadk7SyYN3X+ruE4i6VW8FHg2fN2kBJZHOr8DMeiQ8Ul0RsxU4HTjezH4Zyu4GLjezYyzSy8zONLPddjGmvwPXm1k/M+sL3EA0xhBb9i0zG2nRYPvPgdfdfSXwFHCYmX0t1OMHQLoT9kNEYw/n8XlXFmZ2vpkNDC83EZ2s6psR+9+IxjVOTyh7K8Q1MgyA39iM/aRzhZkNtOjCgElEXV6w6/8mTwMHmtkFZtbNzL4BHAo8mWa7ZD4hes+GJZTdCVxnZocBmNkeZnZ+E/vYjeik/wnQzcxuAHZPWP4xUGJmqc5VfweuNrOhZtab6PPycOhKa5KZfdPM+rl7PfBZKG7OZ0ASKIl0fk8TdZfEHjemWtHdPwO+BJxhZj9z9znApcAfiE60y4gGkXfVzcAcYAHwNjAvlOHuzwP/TdQVtQ7Yn2gsA3f/FDgf+CVRF9cBRIPOTZka1vso9InHfBF43cwqwjpXuvvydIGHMYwbiAaGY2VLiC4WeJ7oiqN04yvN8RBRK2o58D6fvz+79G/i7huAs4gGsDcQtQLOCu9ti7j7NqLxmlmh+2qMuz9O9K1+spltIbpA4IwmdjMNeBZYQtQVtYOdu7v+Ef5uMLN5Sba/hyixv0R0scIO4PvNrMLpwKLwGfgdMD42ThOuBhvXzP10aRZa9yIiIi2mloiIiGRMSURERDKmJCIiIhlTEhERkYx1+gnQ+vbt6yUlJRltW1lZSa9eXe+ycdW7a1G9u5bm1Hvu3Lmfunu/5uyv0yeRkpIS5syZk9G2ZWVllJaWtm5AHYDq3bWo3l1Lc+ptZg1nNUhJ3VkiIpIxJREREcmYkoiIiGRMSURERDKmJCIiIhlTEhERkYwpiYiItHPlq8v5xcu/oHx1+S6VZ0On/52IiHRd5avLKVtZRmlJKWMHjd2p/MEPHqRwdWGj8mTrp9tXc8rrvZ6auhpe+eAVZq6ayTEDj2Fk8Uhq62upqa+hpq6GOR/O4fW1rzOyeCQH9zuY2vpa3vroLa55/hpq62rplt+N68ddz9A9h/Lep+/xq1m/ora+lm553bh89OUM2G0Ayzct557591BXX0ePbj2YMXFGo7q0JiUREWkVu3qSTVeeuGzckHEc1f8oquqqqK6r5tUPXuWV1a9wVP+jOLTfoVTXVTNv3TyufPZKaupq6JbfjZ+c8BOG9hnKO5+8w62zbqW2vpb7Vt3HpaMupf9u/Vm+aTl/W/A36urryLd8zjzwTPYu2pvq+mrWbV1H2coy6ryOPMtjZPFIigqK2Lh9I+99+h6OYxj79t6XPMtjW802Nu3YFI/bMDztTRrTq66r5oayGxqV19TXcPvs25OuX7ayTElERHbNrn4jd3dq62vZUbuDlz94mZkrZ3JU/6M4pN8h7KjdwZwP53D1tKvjJ+xr/+VaBu8xmHc+eYfbZ99ObX0t+Xn5jD9sPHv33JtVn63iiSVPUOfRCXvMwDH0LOjJJ9s+YcHHC6j3egxjv932I8/yqKqrorK6ksqayozqX11XzaQXJjUqr6mv4Y45dzQqr/VaZiyfQZ+iPnTP787Wqq3UeR0A9V7Pxu0bGVY0jNr62p2SQ3HvYkbtO4qF6xfyxodvxJPLCSUnUDqklFdXv8r05dNxnDzyOPvgs/nKgV+hIK+Ap5Y+xT8W/YN66smzPC464iImHjGR9z59j6uevSre4vjrOX9l9H6jeXv921z4fxdSU1dDQX4BT0x4gmMHHcvcD+dy2gOnUV1XTff87pSWlGb0njWXkohIjmXyjfzVD17lhZUvcPSAoxmxzwi21WxjW802Xl/zOrPXzuagvgdR0qeEbTXbWLR+Ef/z2v/ET+TnHXIeexbtyarNq5i2bBp1Xsc9K+/h8H0OpyC/gI3bN7Ji04r4ybFHfg+q66up9+bdOba6rpqbXrqpUXltfS1/X/h3ehb0pN7r4yflOq9j+ablDOkzhM07NseP4zh9e/blyP5HUphfyIKPF/DamtfiJ+ZT9z+V0/Y/jbKVZTyx5In4iXnC4ROYMGICyzYu47+e/y9q6msoyCvgz1/5M18c8EUWrl/INx//JtW11RR2K+SpC55i3JBxzF4zm1P+dkr85Pvcvz0Xf9/LV5dz8v0nx5c99PWHGDtobKPyO758R9Lyn5/083j5yx+8HC+/5thr4scYvtdwpi6eGl926ahLGTtoLKUlpRxRfESjz8IBex/ACxNfaFQ+bsg4ZkyckfKz09o6/Z0NR48e7Zo7q2VU7/Sa23d+zMBj2Fazja1VW3lp1Uu88sErHNrvUEr6lFBRXcH8j+Zz26u3xU/wXzv4a+xeuDurNq9ixooZ1Hv0rbSkTwmGUVlTyZaqLWyr2ZZxPQvyCtijxx7U1NWwuWpzvHxon6Ec3PdgPtj8AYs+WQRE3TDHDT6OE4acQI9uPZi1ehbPLns2fsIeP2I8Fxx+Ae9vep9rpl9DbX0tBXkF3HfufYwZNIa317/N+Y+cHz8xxvrnG55k05XH3tdMtkmVoO958R4uOfGSNhsT2ZVjtKZmzp01191HN2d/SiJN0Mm082nuSaW2vpYtVVt4YcULzFw5k4P7HsygPQaxecdm5n80//MuGsvn5GEnU9itkK1VW/lw64cs2bAk4/7v7vnd2atoL6rrqtm4fWO8/JC+h3Bk/yPpVdCLResXUb6mPP6N/JyDzuFrh3yNZ5c9y+SFk6mnnnzL53tHf48fHPMDFq1fxDce/UbKk29VbRWF3Qp36USe7r3N9kk2k5NvZ/6cN0VJpIWURFquo9Q73clmxooZHNn/SPbfc382bt/IrA9mcf0L11NTX0N+Xj7nHnQuRQVFLN+0nPI15fF++B7derC9dnuz49izx54M3mMwuxfuzseVH7NkwxIg+hZfWlLKGcPPYNbqWTyx+Il4f/dlR13GFV+8giUblsT7tTv6N/KOpqN8zltbaycRjYlIu9HUie7FlS8yer/RlPQp4ZPKT3h51cvcUHZDvBvo5KEnY2Z8uu1T1m5Zy7qKdU0eq7a+lieWPEH/3fpTVVu1U3//qP6jOHX/U3lj7Rs8tfSpqOvG8vju6O9y1ZirWLxh8U5dNE9d8FTKE/wtJ93C2EFjOW71cTz3/nPx8olfmMiIfUYwYp8RSfu1xw4am7JfO9WydNskO+GPHTSWqsFVjZalWj/dMul6lESkzSUmi1H9R7GuYh3Pvf8c33/m+9TURa2E04efjruzbOOytN1DtfW1zF47m/332p++PftSW1/LRxUfxfvtzz/sfL418lus3bqWK56+IuW3/li3zm1fui1ePmPFjPiJ/8LDL2T4XsMZvtfwNjvBt/RErhO8tDUlEcmashXRVTMlfUrYrXA3Vm9ezZwP5/Dk0iebvNKntr6WmStnMnyv4SR2txrGeYeex7+P+nc+3Poh333qu/GE0FRr4MpjrowvO6TvISlP8A27dTI58Te1TCd46YyURKRZknU11dXXMWXxFJ5a8hT9evUj3/JZ8dkKVny2gsWfLt7px1YxvQp6xROIYZw89GQmHD6BzTs2c92M66itr6V7fnemfXNa0n7+q8dcHT/+QXsf1KLWQGxZa3XriIiSiDSQ7Mdnzy59lnMfPpfqumry8/IZO3AsG7ZvYOmGpdTU18S3zSOPIX2GUNKnhGF7DmPeunnx8YSrx1zNzSfdzJvr3twpKdx04k3x44wZOKZVuoHSLROR1qMkInEzls/gzIfOpLqumntX3ssR+x7Bmi1r+Ljy4/g6tfW1LN6wmLEDx7J30d688sErOE6+5fPT0p8y6fjoV8ENWxBfP+Tr9OjWo9XHBkQkt9pdEjGzG4FLgU9C0Y/d/emw7Drg20Ad8AN3n5aTIDuBacum8ciiRyjqVsTGHRuZt24eizcsji+v9Vo+rvyYLx/wZXoV9OLueXfHu5r++Y1/Ju1qOmnoSfHtM21BiEjH0u6SSPA/7v7/EgvM7FBgPHAYsB/wvJkd6B7mTpBGYuMYJww5gb69+jLrg1nMWj2L55c/z6rNq+LrFfcqZszAMYwbPI77F9xPbV0thd0KeeS8R+In+wsOv6BFXU2x5UoWIp1be00iyZwDTHb3KmCFmS0DjgayP2F+B+Pu/GPRP/jm49/cacwCoh/G7dNrn/isovmWz5XHXMl1464D4JIjL0n64zN1NYlIMu3uF+uhO+tiYAswB/iRu28ysz8Ar7n7A2G9vwDPuPujSfZxGXAZQHFx8VGTJ0/OKJaKigp69+6d0bZtadHmRczeNJvCvEI+3vExb2x6g3U7dv6x3bi+47ik5BIG9xzMu1ve5UcLfhSfmO7XX/g1h+1xWHzdjlLv1qZ6dy2qd2onnnhi+572xMyeB/ZNsmgS8BrwKeDAz4D+7n5JS5JIos487cnmHZu57dXb+MUrv4hfNlvUrYgv7f8lDtjrAP74xh8b/bAupqmpK9p7vbNF9e5aVO/U2v20J+5+SnPWM7O7gSfDy7XAoITFA0NZl1G+upznlj9HvuUzd91cnl76NNV11fHleZbHj8f9mOuPvx6Arx/ydY1XiEhWtbsxETPr7+6xvpivAgvD86nAQ2b2G6KB9QOA2TkIMSceeOsBLp5ycfweDHsX7c13R3+XEf1G8INnfxC/QurkoSfHt1GiEJFsa3dJBPiVmY0k6s5aCXwHwN0XmdkjwDtALXBFZ78yy92Z9v40flP+G6Yvnx4vj/14L/abjMP2OaxTzKoqIh1Pu0si7v5vTSy7BbilDcPJibKVZfz+9d/z5kdvsvKzlfTv3Z/Lj7qc+966L+VvMpQ8RCQX2l0S6crcnZtfupkbym4Aorml/vv4/+b646+Ppg8/YqJaHCLSriiJtBOLP13MFU9fwYwVM+JleZZHUbciuud3B9TiEJH2R0kkh8pXl/Pc+8/x/qb3mbxwMj0LevLDMT/kf+f8b7zbqrSkNNdhioikpCSSI+Wryym9rzR+ie4Zw8/gr+f8leLexZx36HnqthKRDkFJJAfcnUkvTIonkDzLY9zgcRT3LgbUbSUiHYeSSBurqavhO09+hxdXvki+5QOo20pEOiwlkTZUUV3Bv/7jX3lm2TP85ISfcOqwU5m5aqa6rUSkw1ISaSPrK9dz5kNnMm/dPO466y4uPepSAI4dfGyOIxMRyZySSJaVry7nsXcfY/LCyWzcvpEp46dw1oFn5TosEZFWoSSSReWryznp/pPYUbsDgLvPulsJREQ6lbxcB9CZvbDihXgCybd8Ptn2SZotREQ6FiWRLFqzZQ0AeeTpCiwR6ZTUnZUlC9cv5J7593BSyUmcMuwUXYElIp2SkkgW1NTVcPE/L2aPwj2YfN5k+vXql+uQRESyQkkkC26ddStz183l0fMfVQIRkU5NYyKtbMHHC7hp5k2MHzGerx/69VyHIyKSVUoiraimroaL/nkRexbtye1n3J7rcEREsk5JpJWUry7njAfPYP5H87nzzDvp27NvrkMSEck6jYm0gvLV5Zx434lU1VWRb/ns23vfXIckItIm1BJpBWUry6iqq9rptYhIV6Ak0gqOG3wcEN0TXT8qFJGuREmkFdR5HQAXHH4BMybO0I8KRaTL0JhIK5jy3hQK8wu586w76d29d67DERFpM2qJ7CJ3Z8riKZw87GQlEBHpcpREdtGiTxax4rMVnHPQObkORUSkzSmJ7KKpi6cC6D4hItIlKYnsoimLp3D0gKPZb7f9ch2KiEibUxLZBeu2rmP22tmcfeDZuQ5FRCQnlER2wRNLngDgnIM1HiIiXVNOkoiZnW9mi8ys3sxGN1h2nZktM7PFZnZaQvnpoWyZmV3b9lE3NnXxVIb2Gcph/Q7LdSgiIjmRq5bIQuBrwEuJhWZ2KDAeOAw4HbjDzPLNLB/4I3AGcCgwIaybMxXVFTy//HnOOegczCyXoYiI5ExOfmzo7u8CyU6+5wCT3b0KWGFmy4Cjw7Jl7r48bDc5rPtO20Tc2PT3p1NVV8XZB2k8RES6rvY2JjIAWJ3wek0oS1WeM1MWT2HPHnvG580SEemKstYSMbPngWRzok9y9ynZOm449mXAZQDFxcWUlZVltJ+Kioqk29Z5HY8vepyj9zqaWS/P2oVI26dU9e7sVO+uRfVuHVlLIu5+SgabrQUGJbweGMpoojzZse8C7gIYPXq0l5aWZhAKlJWVkWzbl1e9zJaXtnDZ8ZdRelhm+27PUtW7s1O9uxbVu3W0t+6sqcB4Mys0s6HAAcBs4A3gADMbambdiQbfp+YsyMVTKcgr4LThp6VfWUSkE8vJwLqZfRW4HegHPGVm8939NHdfZGaPEA2Y1wJXuEfzrJvZ94BpQD5wj7svykXssQkXTxp6ErsX7p6LEERE2o1cXZ31OPB4imW3ALckKX8aeDrLoaW1eMNilm5cylVjrsp1KCIiOdfeurPavd+//nsABuyW04vDRETaBSWRFihfXc6f5v4JgAmPTaB8dXmOIxIRyS0lkRYoW1lGvdcDUF1XTdnKstwGJCKSY0oiLVBaUooR/cq+e353SktKcxuQiEiOKYm0wNhBY+nXsx+j+o9ixsQZjB00NtchiYjklJJIC9XU13DswGOVQEREUBJpsYrqCnp175XrMERE2gUlkRaorqumpr6G3t175zoUEZF2QUmkBSqrKwHoVaCWiIgIKIm0SGVNlETUEhERiSiJtEBFdQWgJCIiEqMk0gLx7iwNrIuIAEoiLaKWiIjIzpREWiCWRDSwLiISURJpAQ2si4jsTEmkBeItEY2JiIgASiItEhtYV0tERCSiJNICGlgXEdlZk7fHNbMfNrXc3X/TuuG0b5U1leRZHoX5hbkORUSkXUh3j/Xdwt+DgC8CU8PrrwCzsxVUe1VRXUHv7r0xs1yHIiLSLjSZRNz9pwBm9hIwyt23htc3Ak9lPbp2pqK6Qpf3iogkaO6YSDFQnfC6OpR1KZU1lRoPERFJkK47K+Z+YLaZPQ4YcA5wb7aCaq9i3VkiIhJpVhJx91vM7BlgHODAt9z9zaxG1g5VVlfqNyIiIglacolvHVCf8Ohy1BIREdlZs5KImV0JPAj0BfYBHjCz72czsPaosqZSA+siIgmaOybybeAYd68EMLNbgXLg9mwF1h6pJSIisrPmdmcZUXdWTF0o61Iqq9USERFJ1NyWyF+B1xtcnfWXrEXVTqklIiKys+ZenfUbMysDjqOLXp1VW19LVV2VkoiISIKWXp3l4bFLV2eZ2flmtsjM6s1sdEJ5iZltN7P54XFnwrKjzOxtM1tmZr+3Np57RLfGFRFpLFdXZy0Evga8lGTZ++4+MjwuTyj/X+BS4IDwOH0Xjt9imsFXRKSxnFyd5e7vhv00a30z6w/s7u6vhdf3A+cCz2Ry/EzE7mqogXURkc81N4m05dVZQ83sTWALcL27vwwMANYkrLMmlCVlZpcBlwEUFxdTVlaWUSAVFRXxbZdsXQLAiiUrKNuY2f46isR6dyWqd9eiereOTK7OgqgV0OTVWWb2PLBvkkWT3H1Kis3WAYPdfYOZHQX808wOa2aMce5+F3AXwOjRo720tLSluwCgrKyM2Lb5q/JhHowZNYbSYZntr6NIrHdXonp3Lap362jJ1VkzgX8JRWmvznL3U1oajLtXAVXh+Vwzex84EFgLDExYdWAoazMaExERaay5LRGA+UQthW4AZjbY3T9ozWDMrB+w0d3rzGwY0QD6cnffaGZbzGwM8DowkTb+tbySiIhIY81KIuFKrJ8AH/P5eIgDX8jkoGb2VaIk0A94yszmu/tpwPHATWZWQ3QZ8eXuvjFs9h9E088XEQ2ot9mgOmhgXUQkmea2RK4EDnL3Da1xUHd/HHg8SfljwGMptpkDjGiN42dCLRERkcaa+2PD1cDmbAbS3unHhiIijTXZEjGzH4any4EyM3uKMPAN0YB7FmNrVyqqKzCMom5FuQ5FRKTdSNedtVv4+0F4dA+PLqeyJrqrYRvPtiIi0q41mUTc/adtFUh7pxl8RUQaS9ed9Vt3v8rMniC6Gmsn7n521iJrZ5REREQaS9ed9bfw9/9lO5D2TrfGFRFpLF131tzwd2bbhNN+qSUiItJYuu6st0nSjUX4saG7Z/Rjw46osrqSPXrskeswRETalXTdWWe1SRQdQEV1BQN2TzlxsIhIl5SuO2tV7LmZDQEOcPfnzawo3badjcZEREQaa+6dDS8FHgX+FIoGAv/MUkztksZEREQaa+60J1cQTQO/BcDdlxLdJrfLUBIREWmsuUmkyt2rYy/MrBvJB9w7pbr6OnbU7lB3lohIA81NIjPN7MdAkZl9CfgH8ET2wmpfYtPAqyUiIrKz5iaRa4FPgLeB7wBPu/ukrEXVzmgGXxGR5Jp7hdWN7n4DcDeAmeWb2YPufmH2Qms/dC8REZHkmtsSGWRm1wGYWXeiG0ctzVpU7Yy6s0REkmtuErkEODwkkieBme5+Y9aiamdiLRENrIuI7CzdtCejEl7+juh3IrOIBtpHufu8bAbXXqg7S0QkuXRjIr9u8HoTcGgod+CkbATV3mhgXUQkuXTTnpzYVoG0Z2qJiIgkl64765vu/kDCvdZ30lXusR4bWNeYiIjIztJ1Z8XOmrslWdZlfrGuloiISHLpurP+FP42ute6mV2VpZjancrqSgyjqKAo16GIiLQrzb3EN5mkXVydUUV1BT0LepJnu/J2iYh0PrtyVrRWi6Kd0wy+IiLJ7UoS6TJjIpU1lbq8V0QkiXRXZ20l9T3Wu8wAgVoiIiLJpRtYT3ZVVpejW+OKiCSnkeJmUEtERCS5nCQRM7vNzN4zswVm9riZ9UlYdp2ZLTOzxWZ2WkL56aFsmZld25bxVlZXKomIiCSRq5bIdGCEu38BWALEppk/FBgPHAacDtwR7l2SD/wROINo7q4JYd02UVFdoYF1EZEkcpJE3P05d68NL18DBobn5wCT3b3K3VcAy4Cjw2OZuy8P93qfHNZtExXVFfQuUEtERKSh5t7ZMJsuAR4OzwcQJZWYNaEMYHWD8mNS7dDMLgMuAyguLqasrCyjwCoqKigrK2Prjq1s/HhjxvvpaGL17mpU765F9W4dWUsiZvY8sG+SRZPcfUpYZxJQCzzYmsd297uAuwBGjx7tpaWlGe2nrKyM4084nh0zd3DI/oeQ6X46mrKysi5T10Sqd9eiereOrCURdz+lqeVmdjFwFnCyu8d+i7IWGJSw2sBQRhPlWbWtZhugGXxFRJLJ1dVZpwPXAGe7+7aERVOB8WZWaGZDgQOA2cAbwAFmNjTc4318WDfrNIOviEhquRoT+QNQCEw3M4DX3P1yd19kZo8A7xB1c13h7nUAZvY9YBqQD9zj7ovaItDYXQ2VREREGstJEnH34U0suwW4JUn508DT2YwrmVhLRJf4iog0pl+spxG7q6FaIiIijSmJpBFviWhgXUSkESWRNDSwLiKSmpJIGhpYFxFJTUkkDQ2si4ikpiSShgbWRURSUxJJI9YS6VnQM8eRiIi0P0oiaVRWV9KzoCd5prdKRKQhnRnTqKiu0OW9IiIpKImkUVGjW+OKiKSiJJKGbo0rIpKakkgaujWuiEhqSiJpVNaoJSIikoqSSBoaWBcRSU1JJA2NiYiIpKYkkoZaIiIiqSmJpFFRrUt8RURSURJpQr3Xs61mm5KIiEgKSiJNqKqvwnFd4isikoKSSBN21O0ANIOviEgqSiJN2F63HdCtcUVEUlESaYJaIiIiTVMSaUK8JaIxERGRpJREmhBLImqJiIgkpyTShB316s4SEWmKkkgTNLAuItI0JZEmqDtLRKRpSiJNiF2dpYF1EZHklESaoO4sEZGm5SSJmNltZvaemS0ws8fNrE8oLzGz7WY2PzzuTNjmKDN728yWmdnvzcyyHef2uu0UdSsiPy8/24cSEemQctUSmQ6McPcvAEuA6xKWve/uI8Pj8oTy/wUuBQ4Ij9OzHeT2+u3qyhIRaUJOkoi7P+futeHla8DAptY3s/7A7u7+mrs7cD9wbnajjMZENKguIpJat1wHAFwCPJzweqiZvQlsAa5395eBAcCahHXWhLKkzOwy4DKA4uJiysrKMgpsa9VWqCbj7TuqioqKLldnUL27GtW7dWQtiZjZ88C+SRZNcvcpYZ1JQC3wYFi2Dhjs7hvM7Cjgn2Z2WEuP7e53AXcBjB492ktLSzOoAdQuqKV4z2Iy3b6jKisr63J1BtW7q1G9W0fWkoi7n9LUcjO7GDgLODl0UeHuVUBVeD7XzN4HDgTWsnOX18BQllXb67azT/d9sn0YEZEOK1dXZ50OXAOc7e7bEsr7mVl+eD6MaAB9ubuvA7aY2ZhwVdZEYEq249xRrzEREZGm5GpM5A9AITA9XKn7WrgS63jgJjOrAeqBy919Y9jmP4B7gSLgmfDIqu1125VERESakJMk4u7DU5Q/BjyWYtkcYEQ242poe912/dBQRKQJ+sV6E3SJr4hI05REUnB3tURERNJQEklhR+0OHFdLRESkCUoiKVRUVwCawVdEpClKIilU1lQCupeIiEhTlERSiLVElERERFJTEkkh3p2lgXURkZSURFKorFZ3lohIOkoiKWhgXUQkPSWRFDSwLiKSnpJIChoTERFJT0kkBY2JiIikpySSgsZERETSUxJJobKmkgIroFtee7iDsIhI+6QkkkJFdQVF+UW5DkNEpF1TEklBSUREJD0lkRQqayrpkd8j12GIiLRrSiIpqCUiIpKekkgKldWVSiIiImkoiaSgloiISHpKIilU1lTSI09jIiIiTVESSWHj9o2s3b6W8tXluQ5FRKTdUhJJonx1OZ9u+5TFFYs5+f6TlUhERFJQEkmibGVZ/Hl1XfVOr0VE5HNKIkmUlpRS1K2IPPLont+d0pLSXIckItIuKYkkMXbQWGZMnMElQy9hxsQZjB00NtchiYi0S5pdMIWxg8ZSNbhKCUREpAlqiYiISMaUREREJGNKIiIikjElERERyZiSiIiIZExJREREMmbunusYssrMPgFWZbh5X+DTVgyno1C9uxbVu2tpTr2HuHu/5uys0yeRXWFmc9x9dK7jaGuqd9eienctrV1vdWeJiEjGlERERCRjSiJNuyvXAeSI6t21qN5dS6vWW2MiIiKSMbVEREQkY0oiIiKSMSWRJMzsdDNbbGbLzOzaXMezq8zsHjNbb2YLE8r2MrPpZrY0/N0zlJuZ/T7UfYGZjUrY5qKw/lIzuygXdWkJMxtkZi+a2TtmtsjMrgzlnbruZtbDzGab2Vuh3j8N5UPN7PVQv4fNrHsoLwyvl4XlJQn7ui6ULzaz03JUpRYxs3wze9PMngyvu0q9V5rZ22Y238zmhLLsf9bdXY+EB5APvA8MA7oDbwGH5jquXazT8cAoYGFC2a+Aa8Pza4Fbw/MvA88ABowBXg/lewHLw989w/M9c123NPXuD4wKz3cDlgCHdva6h/h7h+cFwOuhPo8A40P5ncB3w/P/AO4Mz8cDD4fnh4bPfyEwNPy/yM91/ZpR/x8CDwFPhtddpd4rgb4NyrL+WVdLpLGjgWXuvtzdq4HJwDk5jmmXuPtLwMYGxecA94Xn9wHnJpTf75HXgD5m1h84DZju7hvdfRMwHTg968HvAndf5+7zwvOtwLvAADp53UP8FeFlQXg4cBLwaChvWO/Y+/EocLKZWSif7O5V7r4CWEb0/6PdMrOBwJnAn8NrowvUuwlZ/6wriTQ2AFid8HpNKOtsit19XXj+EVAcnqeqf4d+X0JXxZFE38o7fd1Dl858YD3RieB94DN3rw2rJNYhXr+wfDOwNx2w3sBvgWuA+vB6b7pGvSH6ovCcmc01s8tCWdY/67o9ruDubmad9lpvM+sNPAZc5e5boi+bkc5ad3evA0aaWR/gceDg3EaUfWZ2FrDe3eeaWWmOw8mF49x9rZntA0w3s/cSF2brs66WSGNrgUEJrweGss7m49B8JfxdH8pT1b9Dvi9mVkCUQB509/8LxV2i7gDu/hnwIjCWqMsi9sUxsQ7x+oXlewAb6Hj1/hfgbDNbSdQNfRLwOzp/vQFw97Xh73qiLw5H0wafdSWRxt4ADghXdHQnGnCbmuOYsmEqELvy4iJgSkL5xHD1xhhgc2gOTwNONbM9wxUep4aydiv0b/8FeNfdf5OwqFPX3cz6hRYIZlYEfIloPOhF4LywWsN6x96P84AXPBplnQqMD1cxDQUOAGa3SSUy4O7XuftAdy8h+n/7grtfSCevN4CZ9TKz3WLPiT6jC2mLz3quryhojw+iKxeWEPUjT8p1PK1Qn78D64Aaoj7ObxP1/c4AlgLPA3uFdQ34Y6j728DohP1cQjTIuAz4Vq7r1Yx6H0fUT7wAmB8eX+7sdQe+ALwZ6r0QuCGUDyM6GS4D/gEUhvIe4fWysHxYwr4mhfdjMXBGruvWgveglM+vzur09Q51fCs8FsXOW23xWde0JyIikjF1Z4mISMaUREREJGNKIiIikjElERERyZiSiIiIZExJRDoUM3Mz+3XC6/80sxtbad/3mtl56dfc5eOcb2bvmtmLDcr3M7NHw/ORZvblVjxmHzP7j2THEtkVSiLS0VQBXzOzvrkOJFHCL6Kb49vApe5+YmKhu3/o7rEkNpLoNy2tFUMfollrkx1LJGNKItLR1BLdI/rqhgsatiTMrCL8LTWzmWY2xcyWm9kvzexCi+658baZ7Z+wm1PMbI6ZLQlzMcUmM7zNzN4I9174TsJ+XzazqcA7SeKZEPa/0MxuDWU3EP0I8i9mdluD9UvCut2Bm4BvWHRviG+EXyTfE2J+08zOCdtcbGZTzewFYIaZ9TazGWY2Lxw7NgP1L4H9w/5uix0r7KOHmf01rP+mmZ2YsO//M7NnLbq3xK8S3o97Q6xvm1mjfwvpOjQBo3REfwQWxE5qzXQEcAjRlPjLgT+7+9EW3ajq+8BVYb0SojmH9gdeNLPhwESiaSG+aGaFwCwzey6sPwoY4dGU4XFmth9wK3AUsIlodtVz3f0mMzsJ+E93n5MsUHevDslmtLt/L+zv50TTclwSpjSZbWbPJ8TwBXffGFojX/Voosm+wGshyV0b4hwZ9leScMgrosP64WZ2cIj1wLBsJNHsx1XAYjO7HdgHGODuI8K++jTxvksnp5aIdDjuvgW4H/hBCzZ7w6P7i1QRTfUQSwJvEyWOmEfcvd7dlxIlm4OJ5g+aaNHU6q8TTSVxQFh/dsMEEnwRKHP3TzyaZvxBopuDZepU4NoQQxnRlB2Dw7Lp7h67X4wBPzezBUTTXAzg8+m/UzkOeADA3d8DVgGxJDLD3Te7+w6i1tYQovdlmJndbmanA1t2oV7SwaklIh3Vb4F5wF8TymoJX4zMLI/ozpQxVQnP6xNe17Pz/4OG8wA50Yn5++6+00R0Fk03XplJ8Bkw4OvuvrhBDMc0iOFCoB9wlLvXWDSjbY9dOG7i+1YHdHP3TWZ2BNENjC4H/pVoviXpgtQSkQ4pfPN+hGiQOmYlUfcRwNlEd/RrqfPNLC+MkwwjmoBvGvBdi6aVx8wOtGim1KbMBk4ws75mlg9MAGa2II6tRLf0jZkGfN8suhmKmR2ZYrs9iO6pURPGNoak2F+il4mSD6EbazBRvZMK3WR57v4YcD1Rd5p0UUoi0pH9Gki8SutuohP3W0T3z8iklfABUQJ4Brg8dOP8magrZ14YjP4TaVrxHk2rfS3RNORvAXPdfUpT2zTwInBobGAd+BlRUlxgZovC62QeBEab2dtEYznvhXg2EI3lLGw4oA/cAeSFbR4GLg7dfqkMAMpC19oDwHUtqJd0MprFV0REMqaWiIiIZExJREREMqYkIiIiGVMSERGRjCmJiIhIxpREREQkY0oiIiKSsf8PUjo9ApRp048AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to plot Likelihood v/s Number of Iterations.\n",
    "iters = np.array(range(0,5000,100))\n",
    "plt.plot(iters,log_likelihood_values,'.-',color='green')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title(\"Likelihood vs Number of Iterations.\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluating your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hypothesis(...) to predict.\n",
    "\n",
    "\n",
    "def predict(X_test, w_final):\n",
    "    threshold=0.5\n",
    "    predicted_values=[]\n",
    "    predictions=hypothesis(X_test,w_final)\n",
    "    for i in predictions:\n",
    "        if i >=threshold:\n",
    "            predicted_values.append(1)\n",
    "        else:\n",
    "            predicted_values.append(0)\n",
    "    return predicted_values\n",
    "\n",
    "\n",
    "predicted_values=predict(X_test,w_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "Precision:  0.9887640449438202\n",
      "Recall:  0.9887640449438202\n",
      "F1:  0.9887640449438202\n",
      "Confusion Matrix: \n",
      "TP:  88  FN:  1  FP:  1  TN:  53\n"
     ]
    }
   ],
   "source": [
    "TP=0\n",
    "FP=0\n",
    "FN=0\n",
    "TN=0\n",
    "\n",
    "print(y_test.shape[0])\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i]==0 and predicted_values[i]==0:\n",
    "        TN=TN+1\n",
    "    if y_test[i]==0 and predicted_values[i]==1:\n",
    "        FP=FP+1\n",
    "    if y_test[i]==1 and predicted_values[i]==1:\n",
    "        TP=TP+1\n",
    "    if y_test[i]==1 and predicted_values[i]==0:\n",
    "        FN=FN+1\n",
    "    # count TP,FP,FN,FP\n",
    "        \n",
    "# calculate precision, recall and f1\n",
    "precision= TP/(TP+FP)\n",
    "print(\"Precision: \",precision)\n",
    "recall=TP/(TP+FN)\n",
    "print(\"Recall: \",recall)\n",
    "f1=2*(precision*recall)/(precision+recall)\n",
    "print(\"F1: \",f1)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(\"TP: \",TP,\" FN: \",FN,\" FP: \",FP,\" TN: \",TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Report\n",
    "\n",
    "Best Learning rate = 0.1 \n",
    "That learning rate gave us the least amount of false positives and false negatives.\n",
    "Increasing the number of iterations didn't change the results in any significant way so I concluded that after a certain value the change is minimal. \n",
    "\n",
    "## To run the program:\n",
    "1. Run the last block of code to display the precision, recall, F1, and the confusion matrix.\n",
    "2. Run the predict function to predict if a tumor is benign or malignant.\n",
    "\n",
    "## Detailed description:\n",
    "\n",
    "1. I import tha dataset and and split it.\n",
    "2. I initialize w with zeros and run my hypothesis function.\n",
    "3. I run my gradient ascent function 5000 times to reach my final w values. \n",
    "4. I print the log likelihood values every 100 steps.\n",
    "5. I use the final w values and run my hypothesis function again to find my predictions which I then compare to my threshold.\n",
    "6. I use the predicted values to build my confusion matrix and find all my accuracy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
