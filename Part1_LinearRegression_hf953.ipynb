{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "# Load dataset to some variable \n",
    "# boston_data = .....\n",
    "\n",
    "boston_data = load_boston()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n",
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "# X = boston_data.....\n",
    "# y = boston_data.....\n",
    "X, y = load_boston(return_X_y=True)\n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('The number of features is: ', X.shape[1])\n",
    "print('The features: ', boston_data.feature_names)\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "print(X[0:2])\n",
    "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "# print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "# print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "# print('The number of exampels in our dataset: ', X.shape[0])\n",
    "# Observing the first 2 rows of the data\n",
    "# print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
    "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
    "# Simply copy Y into Y_2 \n",
    "\n",
    "# X_2 = ....\n",
    "# y_2 = ....\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)\n",
    "# print(X[:,0])\n",
    "# plt.figure()\n",
    "# plt.scatter(X[:,0],y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    # use np.linalg.pinv(...)\n",
    "    m,n = X_train.shape\n",
    "    I = np.eye(n)\n",
    "    w = np.dot(np.linalg.pinv(np.dot(X_train.T, X_train)+alpha*I), np.dot(X_train.T, y_train))\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "\n",
    "def get_coeff_linear_normaleq(X_train, y_train):\n",
    "    # use np.linalg.pinv(...)\n",
    "    w = np.dot(np.linalg.pinv(np.dot(X_train.T, X_train)), np.dot(X_train.T, y_train))\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_err_ridge function.\n",
    "# Return the train_error and test_error values\n",
    "#     return train_error, test_error\n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "\n",
    "    y_pred=np.dot(X_train,w)\n",
    "    MSE_train = np.mean((y_pred-y_train)**2)\n",
    "    \n",
    "    y_pred=np.dot(X_test,w)\n",
    "    MSE_test = np.mean((y_pred-y_test)**2)\n",
    "    return MSE_test , MSE_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish writting the k_fold_cross_validation function. \n",
    "# Returns the average training error and average test error from the k-fold cross validation\n",
    "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha=None):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        \n",
    "        # Subtract y_train_mean from y_train and y_test\n",
    "        y_train_mean=np.mean(y_train)\n",
    "        # y_train_mean = ...\n",
    "        # y_train = ...\n",
    "        # y_test = ...\n",
    "        y_train= y_train-y_train_mean\n",
    "        y_test=y_test-y_train_mean\n",
    "        # Scaling the data matrix\n",
    "        # Using scaler=preprocessing.StandardScaler().fit(...)\n",
    "        # And scaler.transform(...)\n",
    "        # X_train = \n",
    "        # X_test =\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train=scaler.transform(X_train)\n",
    "        X_test=scaler.transform(X_test)\n",
    "        # Determine the training error and the test error\n",
    "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
    "        # And use evaluate_err()\n",
    "        if alpha==None:\n",
    "            w=get_coeff_linear_normaleq(X_train,y_train)\n",
    "        else:\n",
    "#             print(alpha,'alpha testing')\n",
    "            w=get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "        \n",
    "        total_E_val_test, total_E_val_train=evaluate_err(X_train, X_test, y_train, y_test, w)\n",
    "        \n",
    "       ##############\n",
    "    return  total_E_val_test, total_E_val_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the error for the both linear regression and ridge regression\n",
    "# the error should include both training error and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to predict the new test case.\n",
    "def predict(X_test):\n",
    "    w_simple= get_coeff_linear_normaleq(X_train,y_train)\n",
    "    w_ridge = get_coeff_ridge_normaleq(X_train, y_train, 10)\n",
    "    predict_simple = np.dot(X_test,w_simple)\n",
    "    predict_ridge = np.dot(X_test,w_ridge)\n",
    "    return predict_simple,predict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Linear Regression Error\n",
      "\n",
      "                         Alpha Value:  10.0\n",
      "\n",
      "Test Error:  28.51372751066697              Training Error:  20.72314093498285\n",
      "\n",
      "                         Alpha Value:  31.622776601683793\n",
      "\n",
      "Test Error:  29.00483225377633              Training Error:  21.138364993622243\n",
      "\n",
      "                         Alpha Value:  100.0\n",
      "\n",
      "Test Error:  30.484127136312544              Training Error:  22.682471987114713\n",
      "\n",
      "                         Alpha Value:  316.22776601683796\n",
      "\n",
      "Test Error:  34.91174932308663              Training Error:  27.496108333619837\n",
      "\n",
      "                         Alpha Value:  1000.0\n",
      "\n",
      "Test Error:  45.35910577785945              Training Error:  38.547243209998605\n",
      "\n",
      "                         Alpha Value:  3162.2776601683795\n",
      "\n",
      "Test Error:  61.023468357328404              Training Error:  54.47066705725698\n",
      "\n",
      "                         Alpha Value:  10000.0\n",
      "\n",
      "Test Error:  75.87357279068969              Training Error:  69.33026881273167\n",
      "\n",
      "                         Alpha Value:  31622.776601683792\n",
      "\n",
      "Test Error:  84.44226016024433              Training Error:  77.90211374617277\n",
      "\n",
      "                         Alpha Value:  100000.0\n",
      "\n",
      "Test Error:  87.9173379555169              Training Error:  81.38309684089121\n",
      "\n",
      "                         Alpha Value:  316227.7660168379\n",
      "\n",
      "Test Error:  89.11649799990956              Training Error:  82.58504937674222\n",
      "\n",
      "                         Alpha Value:  1000000.0\n",
      "\n",
      "Test Error:  89.50670921462685              Training Error:  82.97625622951311\n",
      "\n",
      "                         Alpha Value:  3162277.6601683795\n",
      "\n",
      "Test Error:  89.63123912910783              Training Error:  83.10111297111605\n",
      "\n",
      "                         Alpha Value:  10000000.0\n",
      "\n",
      "Test Error:  89.6707334810929              Training Error:  83.14071189655716\n",
      "\n",
      "Simple Linear Regression Error\n",
      "\n",
      "Test Error:  28.294347280410594              Training Error:  20.62997708335374\n",
      "\n",
      "                            Polynomial features\n",
      "\n",
      "Ridge Linear Regression Error\n",
      "\n",
      "                         Alpha Value:  10.0\n",
      "\n",
      "Test Error:  17.341076779155898              Training Error:  9.459927981776927\n",
      "\n",
      "                         Alpha Value:  31.622776601683793\n",
      "\n",
      "Test Error:  20.148344225474425              Training Error:  12.316203122192675\n",
      "\n",
      "                         Alpha Value:  100.0\n",
      "\n",
      "Test Error:  23.928873870983708              Training Error:  15.566789681042158\n",
      "\n",
      "                         Alpha Value:  316.22776601683796\n",
      "\n",
      "Test Error:  27.04287833638706              Training Error:  18.786908458110105\n",
      "\n",
      "                         Alpha Value:  1000.0\n",
      "\n",
      "Test Error:  30.95738191600869              Training Error:  23.54628096262264\n",
      "\n",
      "                         Alpha Value:  3162.2776601683795\n",
      "\n",
      "Test Error:  39.678639202914354              Training Error:  32.922618786957116\n",
      "\n",
      "                         Alpha Value:  10000.0\n",
      "\n",
      "Test Error:  53.76371595400008              Training Error:  47.055225206661206\n",
      "\n",
      "                         Alpha Value:  31622.776601683792\n",
      "\n",
      "Test Error:  69.14056419608896              Training Error:  62.33909148948752\n",
      "\n",
      "                         Alpha Value:  100000.0\n",
      "\n",
      "Test Error:  80.7392870647847              Training Error:  74.02575620774886\n",
      "\n",
      "                         Alpha Value:  316227.7660168379\n",
      "\n",
      "Test Error:  86.46664691683608              Training Error:  79.85955034767595\n",
      "\n",
      "                         Alpha Value:  1000000.0\n",
      "\n",
      "Test Error:  88.6230985222036              Training Error:  82.06629228370939\n",
      "\n",
      "                         Alpha Value:  3162277.6601683795\n",
      "\n",
      "Test Error:  89.34695894876496              Training Error:  82.80823483884315\n",
      "\n",
      "                         Alpha Value:  10000000.0\n",
      "\n",
      "Test Error:  89.58034054307839              Training Error:  83.04757289286185\n",
      "\n",
      "Simple Linear Regression Error\n",
      "\n",
      "Test Error:  12.308057693832541              Training Error:  5.171135198614792\n"
     ]
    }
   ],
   "source": [
    "# print(predict(X_test))\n",
    "alpha =  np.logspace(1, 7, num=13)\n",
    "print('Ridge Linear Regression Error')\n",
    "print()\n",
    "for i in alpha:\n",
    "    print('                         Alpha Value: ',i)\n",
    "    print()\n",
    "    total_E_val_test, total_E_val_train = k_fold_cross_validation(5, X, y, i)\n",
    "    print('Test Error: ',total_E_val_test, '             Training Error: ',total_E_val_train)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print('Simple Linear Regression Error')\n",
    "print()\n",
    "total_E_val_test, total_E_val_train = k_fold_cross_validation(5, X, y)\n",
    "print('Test Error: ',total_E_val_test, '             Training Error: ',total_E_val_train)\n",
    "\n",
    "print()\n",
    "print('                            Polynomial features')\n",
    "print()\n",
    "\n",
    "print('Ridge Linear Regression Error')\n",
    "print()\n",
    "for i in alpha:\n",
    "    print('                         Alpha Value: ',i)\n",
    "    print()\n",
    "    total_E_val_test, total_E_val_train = k_fold_cross_validation(5, X_2, y_2, i)\n",
    "    print('Test Error: ',total_E_val_test, '             Training Error: ',total_E_val_train)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print('Simple Linear Regression Error')\n",
    "print()\n",
    "total_E_val_test, total_E_val_train = k_fold_cross_validation(5, X_2, y_2)\n",
    "print('Test Error: ',total_E_val_test, '             Training Error: ',total_E_val_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Linear Regression Report\n",
    "\n",
    "If given a choice to implement one of the above models to predict housing prices, I would definitely choose to transform my data using polynomial features. This allows the model to identify non-linear patterns. My test error was the least when I tested my model with polynomial regression. In terms of simple linear or ridge linear regression, for most of the alpha values my model was overfitting. Therefore I concluded that its better to use simple linear regression because it had the lowest test error. \n",
    "\n",
    "## To run the program:\n",
    "\n",
    "1. Run the last block of code to display all testing and training erros using Kfold validation.\n",
    "2. Run the predict function to predict all the house prices for the testing set. Function returns predicted values using simple and ridge regression.\n",
    "\n",
    "## Deatiled description:\n",
    "\n",
    "1. I import tha dataset and and split it.\n",
    "2. I make a instance of the data with polynomial features.\n",
    "3. In the kfold cross validation function, I use sklearn's built in kfold function. I run the simple regression and the ridge regression closed form solutions on the outputs.\n",
    "4. I run evauate error to return the error.\n",
    "5. I run the kfold function and iterate through every alpha value and print out the errors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
